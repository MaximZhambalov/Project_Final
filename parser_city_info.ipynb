{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ПАРСИНГ ИНФОРМАЦИИ ПО ГОРОДАМ\n",
    "\n",
    "Не стоит надеяться, что этот ноутбук будет работать. Конкретно в этом случае парсинг довольно нетривиальная (по крайней мере для меня) задача, поэтому многие вещи приходилось править практически в ручном режиме:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import wikipedia as wp\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_mask(data_, column_, list_, operator='or'):  \n",
    "    if operator == 'or':\n",
    "        mask_full = False\n",
    "        for prop in list_:\n",
    "            mask_ = data_[column_].str.find(prop) + 1\n",
    "            mask_ = mask_.astype(bool)\n",
    "            mask_full = mask_full | mask_\n",
    "        return mask_full\n",
    "    elif operator == 'and':\n",
    "        mask_full = True\n",
    "        for prop in list_:\n",
    "            mask_ = data_[column_].str.find(prop) + 1\n",
    "            mask_ = mask_.astype(bool)\n",
    "            mask_full = mask_full & mask_\n",
    "        return mask_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_state_to_abbrev = {\n",
    "    \"Alabama\": \"AL\",\n",
    "    \"Alaska\": \"AK\",\n",
    "    \"Arizona\": \"AZ\",\n",
    "    \"Arkansas\": \"AR\",\n",
    "    \"California\": \"CA\",\n",
    "    \"Colorado\": \"CO\",\n",
    "    \"Connecticut\": \"CT\",\n",
    "    \"Delaware\": \"DE\",\n",
    "    \"Florida\": \"FL\",\n",
    "    \"Georgia\": \"GA\",\n",
    "    \"Hawaii\": \"HI\",\n",
    "    \"Idaho\": \"ID\",\n",
    "    \"Illinois\": \"IL\",\n",
    "    \"Indiana\": \"IN\",\n",
    "    \"Iowa\": \"IA\",\n",
    "    \"Kansas\": \"KS\",\n",
    "    \"Kentucky\": \"KY\",\n",
    "    \"Louisiana\": \"LA\",\n",
    "    \"Maine\": \"ME\",\n",
    "    \"Maryland\": \"MD\",\n",
    "    \"Massachusetts\": \"MA\",\n",
    "    \"Michigan\": \"MI\",\n",
    "    \"Minnesota\": \"MN\",\n",
    "    \"Mississippi\": \"MS\",\n",
    "    \"Missouri\": \"MO\",\n",
    "    \"Montana\": \"MT\",\n",
    "    \"Nebraska\": \"NE\",\n",
    "    \"Nevada\": \"NV\",\n",
    "    \"New Hampshire\": \"NH\",\n",
    "    \"New Jersey\": \"NJ\",\n",
    "    \"New Mexico\": \"NM\",\n",
    "    \"New York\": \"NY\",\n",
    "    \"North Carolina\": \"NC\",\n",
    "    \"North Dakota\": \"ND\",\n",
    "    \"Ohio\": \"OH\",\n",
    "    \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\",\n",
    "    \"Pennsylvania\": \"PA\",\n",
    "    \"Rhode Island\": \"RI\",\n",
    "    \"South Carolina\": \"SC\",\n",
    "    \"South Dakota\": \"SD\",\n",
    "    \"Tennessee\": \"TN\",\n",
    "    \"Texas\": \"TX\",\n",
    "    \"Utah\": \"UT\",\n",
    "    \"Vermont\": \"VT\",\n",
    "    \"Virginia\": \"VA\",\n",
    "    \"Washington\": \"WA\",\n",
    "    \"West Virginia\": \"WV\",\n",
    "    \"Wisconsin\": \"WI\",\n",
    "    \"Wyoming\": \"WY\",\n",
    "    \"District of Columbia\": \"DC\",\n",
    "    \"American Samoa\": \"AS\",\n",
    "    \"Guam\": \"GU\",\n",
    "    \"Northern Mariana Islands\": \"MP\",\n",
    "    \"Puerto Rico\": \"PR\",\n",
    "    \"United States Minor Outlying Islands\": \"UM\",\n",
    "    \"U.S. Virgin Islands\": \"VI\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_zip = pd.read_csv('data/zipcode_parser_data.csv', index_col='Unnamed: 0', dtype='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Спарсим корректные названия городов с помощью индекса. Испольуем [этот API](https://api-ninjas.com/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipcode 98198 for Seattle resolved succefully! (-4 zipcodes to go!) go!)\r"
     ]
    }
   ],
   "source": [
    "zip_list = data_zip['zipcode'].unique().tolist()\n",
    "city_list = {}\n",
    "city_error_list = {}\n",
    "total = len(zip_list)\n",
    "\n",
    "# for i, zip_ in enumerate(zip_list):   \n",
    "#     try:      \n",
    "#         api_url = 'https://api.api-ninjas.com/v1/zipcode?zip={}'.format(zip_)\n",
    "#         response = requests.get(api_url, headers={'X-Api-Key': '/JkP/JwK9iI5nk/fwSXseQ==7qi1yzX4NLWV561W'})\n",
    "#         city = response.json()[0]['city']\n",
    "#         city_list[zip_] = city\n",
    "#         print(f'Zipcode {zip_} for {city} resolved succefully! ({total - i} zipcodes to go!)', end='\\r')\n",
    "#     except (KeyError, IndexError) as error:\n",
    "#         print(f'An issue occured with zipcode {zip_}')\n",
    "#         city_error_list[zip_] = error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_list = list(city_list.keys())\n",
    "parsed_list = [str(el) for el in parsed_list]\n",
    "zip_list = [el for el in zip_list if el not in parsed_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_error_list = list(city_error_list.keys())\n",
    "parsed_error_list = [str(el) for el in parsed_error_list]\n",
    "zip_list = [el for el in zip_list if el not in parsed_error_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_zip['city_correct'] = data_zip['zipcode'].map(city_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка адреса в википедии на валидность:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# us_state_to_abbrev_swapped = {v: k for k, v in us_state_to_abbrev.items()}\n",
    "# data_zip['state_full_name'] = data_zip['state'].map(us_state_to_abbrev_swapped)\n",
    "# data_zip['city+state'] = data_zip['city_correct'] + ', ' + data_zip['state_full_name']\n",
    "# cities = data_zip['city+state'].unique().tolist()\n",
    "# cities = pd.DataFrame(cities, columns=['city'])\n",
    "# cities['city'] = cities['city'].str.replace(' ', '_')\n",
    "\n",
    "# city_wiki_url = {}\n",
    "# city_url_response = {}\n",
    "# total = cities.shape[0]\n",
    "\n",
    "# for i, city in enumerate(cities['city'].unique().tolist()):\n",
    "#     url = 'https://en.wikipedia.org/wiki/' + city\n",
    "#     city_wiki_url[city] = url\n",
    "#     response = requests.get(url)\n",
    "#     city_url_response[city] = response.status_code\n",
    "#     if code == 200:\n",
    "#         print(f'City: {city}, code={response.status_code}, {total - i} more to go', end='\\r')\n",
    "#     else:\n",
    "#         print(f'Issue with city {city}, code={response.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример парсинга таблицы с википедии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# парсинг таблиц с википедии\n",
    "\n",
    "# html = wp.page(city).html()\n",
    "# try: \n",
    "#     df_largest_cities = pd.read_html(html)[0]\n",
    "# except IndexError:\n",
    "#     df_largest_cities = pd.read_html(html)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем данные таблицы с [этого сайта](https://simplemaps.com/data/us-cities):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_zip = data_zip.drop('city', axis=1)\n",
    "data_zip = data_zip.reset_index()\n",
    "data_zip = data_zip.rename(columns={'city_correct':'city'})\n",
    "\n",
    "cities_info = pd.read_csv('data/us_cities.csv')\n",
    "cities_info = cities_info[['city', 'lat', 'lng', 'population', 'density', 'state_id']]\n",
    "\n",
    "data_info = data_zip.merge(cities_info, how='left', left_on=['city', 'state'], right_on=['city', 'state_id'])\n",
    "data_info = data_info.drop(index=13334, axis=0)\n",
    "\n",
    "data_info = data_info.drop('state_id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Парсим недостающую информацию по городам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Galloway info parsed succefully! (1 cities to go!) go!)!)!)!)!))\r"
     ]
    }
   ],
   "source": [
    "# mask_nan = data_info.lat.isna()\n",
    "\n",
    "# city_list_nan = data_info.city[mask_nan].unique().tolist()\n",
    "# city_data = {}\n",
    "# city_data_errors = {}\n",
    "\n",
    "# total = len(city_list_nan)\n",
    "\n",
    "# for i, city in enumerate(city_list_nan):   \n",
    "#     try:      \n",
    "#         api_url = 'https://api.api-ninjas.com/v1/city?name={}'.format(city)\n",
    "#         response = requests.get(api_url, headers={'X-Api-Key': 'yrRFg510NuWDi9uzvWt85Q==Vr7GuJDVE0PjbsD2'})\n",
    "#         city_resp = response.json()\n",
    "#         city_data[city] = city_resp\n",
    "#         print(f'{city} info parsed succefully! ({total - i} cities to go!)', end='\\r')\n",
    "#     except (KeyError, IndexError) as error:\n",
    "#         print(f'An issue occured with city {city}')\n",
    "#         city_data_errors[city] = error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вставляем значения координат и населения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_us_city = []\n",
    "# idx_error_city = []\n",
    "# for city in city_data.keys():\n",
    "#     try:\n",
    "#         if city_data[city][0]['country'] == 'US':\n",
    "#             data_info.loc[data_info.city == city, 'lat'] = city_data[city][0]['latitude']\n",
    "#             data_info.loc[data_info.city == city, 'lng'] = city_data[city][0]['longitude']\n",
    "#             data_info.loc[data_info.city == city, 'population'] = city_data[city][0]['population']\n",
    "#         else:\n",
    "#             non_us_city.append(city)\n",
    "#     except IndexError as e:\n",
    "#         idx_error_city.append(city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Спарсим аннотацию, таблицы и координаты с википедии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_nan = data_info.lat.isna()\n",
    "# city_list_nan = data_info.loc[mask_nan, 'city+state'].unique().tolist()\n",
    "# total = len(city_list_nan)\n",
    "\n",
    "# city_coords = {}\n",
    "# city_df = {}\n",
    "# city_error = {}\n",
    "\n",
    "# for i, city in enumerate(city_list_nan):\n",
    "#     url = 'https://en.wikipedia.org/wiki/' + city\n",
    "#     response = requests.get(url)\n",
    "#     city_url_response[city] = response.status_code\n",
    "#     if code == 200:\n",
    "#         try:\n",
    "#             page = wp.page(city)\n",
    "#         except (wp.PageError, wp.DisambiguationError) as error:\n",
    "#             city_error[city] = error\n",
    "#             print(f'Issue with city {city}, code={error}')\n",
    "#         try: \n",
    "#             city_df[city] = pd.read_html(page.html())[0]\n",
    "#         except IndexError:\n",
    "#             city_df[city] = pd.read_html(page.html())[1]\n",
    "#         city_coords[city] = page.coordinates\n",
    "#         print(f'City: {city}, code={code}, {total - i} more to go', end='\\r')\n",
    "#     else:\n",
    "#         print(f'Issue with city {city}, code={code}')\n",
    "#         city_error[city] = code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue with Stow, NY city!arsed!!\n",
      "Issue with Arrington, TN city!!sed!\n",
      "Issue with Brant, NY city!\n",
      "Issue with Rowe, VA city!\n",
      "Issue with Camby, IN city!ed!parsed!d!ed!\n",
      "Issue with Playa Vista, CA city!!!!sed!d!\n",
      "Issue with Mc Leansville, NC city!sed!!\n",
      "Issue with Busy, KY city!arsed! parsed!\n",
      "Issue with Drift, KY city! parsed!\n",
      "Issue with Ceresco, MI city!parsed!sed!\n",
      "Issue with Graysville, GA city!!parsed!\n",
      "Issue with Mallie, KY city!d!d!parsed!d!\n",
      "City Topmost, KY info parsed!d!arsed!d!\r"
     ]
    }
   ],
   "source": [
    "# city_coords = {}\n",
    "# city_info = {}\n",
    "# city_error = {}\n",
    "# city_parsed_list = []\n",
    "\n",
    "# for city in city_total_errors_list_state:\n",
    "#     try:\n",
    "#         page = wp.page(city)\n",
    "#         city_coords[city] = page.coordinates\n",
    "#         city_info[city] = page.summary\n",
    "#         city_parsed_list.append(city)\n",
    "#         print(f'City {city} info parsed!', end='\\r')\n",
    "#     except (wp.PageError, KeyError) as e:\n",
    "#         city_error[city] = e\n",
    "#         print(f'Issue with {city} city!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем координаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_info['city_2'] = data_info['city'] + ', ' + data_info['state']\n",
    "\n",
    "# for city in city_coords.keys():\n",
    "#     data_info.loc[data_info.city_2 == city, 'lat'] = float(city_coords[city][0])\n",
    "#     data_info.loc[data_info.city_2 == city, 'lng'] = float(city_coords[city][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ПРАВКА ТАБЛИЦЫ ПО ЛОКАЛЬНЫМ КООРДИНАТАМ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим города, которые упоминаются менее 10 раз:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# найдём количество строк для конкретного города в датасете\n",
    "\n",
    "unique_cities = data_info.loc[data_info.isna().density, 'city_2'].unique().tolist()\n",
    "cities_q = {}\n",
    "\n",
    "for city in unique_cities:\n",
    "    q = data_info[data_info.city_2 == city].shape[0]\n",
    "    if q not in cities_q.keys():\n",
    "        cities_q[q] = []\n",
    "        cities_q[q].append(city)\n",
    "    else:\n",
    "        cities_q[q].append(city) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_to_remove = []\n",
    "for k in range(1, 10):\n",
    "    cities_to_remove.extend(cities_q[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_rows_by_values(data, col, values):\n",
    "    return data[~data[col].isin(values)]\n",
    "\n",
    "data_info = filter_rows_by_values(data_info, 'city', cities_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_total_errors_list = []\n",
    "city_total_errors_list_state = []\n",
    "\n",
    "for city in data_info.city[data_info.density.isna()].unique().tolist():\n",
    "    city_total_errors_list.append(city)\n",
    "\n",
    "for city in data_info.city_2[data_info.density.isna()].unique().tolist():\n",
    "    city_total_errors_list_state.append(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# замена в списках слова Saint - иначе парсер не находит город, или находит, но не тот\n",
    "\n",
    "# city_saint = []\n",
    "# for el in city_total_errors_list:\n",
    "#     if el.find('Saint') != -1:\n",
    "#         city_saint.append(el.replace('Saint ', 'St. '))\n",
    "\n",
    "# city_saint_state = []\n",
    "# for el in city_total_errors_list_state:\n",
    "#     if el.find('Saint') != -1:\n",
    "#         city_saint_state.append(el.replace('Saint ', 'St. '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество строк по этим городам =  1890\n"
     ]
    }
   ],
   "source": [
    "a = list(cities_q.keys())\n",
    "a.sort(reverse=True)\n",
    "cities_manual = []\n",
    "print('Количество строк по этим городам = ', sum(a[:20]))\n",
    "for el in a[:20]:\n",
    "    cities_manual.extend(cities_q[el])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исправим часть ошибок вручную - это проще сделать, чем настраивать парсер для википедии в условиях ограниченного времени:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flushing, NY\n",
      "Jamaica, NY\n",
      "Antioch, TN\n",
      "Hixson, TN\n",
      "La Jolla, CA\n",
      "North Hollywood, CA\n",
      "Hermitage, TN\n",
      "Inlet Beach, FL\n"
     ]
    }
   ],
   "source": [
    "neighboors_1 = {'Flushing, NY': 'New York, NY', 'Jamaica, NY': 'New York, NY', 'Antioch, TN': 'Nashville, TN', 'Hixson, TN': 'Chattanooga, TN',\n",
    "    'La Jolla, CA': 'San Diego, CA', 'North Hollywood, CA': 'Los Angeles, CA', 'Hermitage, TN': 'Nashville, TN', 'Inlet Beach, FL': 'Panama City, FL'\n",
    "}\n",
    "neighboors_2 = {'Kingwood, TX': 'Houston, TX', 'Huffman, TX': 'Houston, TX', 'Old Hickory, TN': 'Nashville, TN', 'Clearwater Beach, FL': 'Tampa, FL',\n",
    "    'Forest Hills, NY': 'New York, NY', 'Madison, TN': 'Nashville, TN', 'Woodland Hills, CA': 'Los Angeles, CA', 'Lake In The Hills, IL': 'Chicago, IL',\n",
    "    'Cordova, TN': 'Memphis, TN', 'Bayside, NY': 'New York, NY', 'Van Nuys, CA': 'Los Angeles, CA', 'Sylmar, CA': 'Los Angeles, CA', 'Astoria, NY':  'New York, NY',\n",
    "    'College Point, NY': 'New York, NY', 'Whitestone, NY': 'New York, NY', 'East Elmhurst, NY': 'New York, NY', 'Corona, NY': 'New York, NY',\n",
    "    'Long Island City, NY': 'New York, NY'\n",
    "}\n",
    "\n",
    "for k, v in neighboors_1.items():\n",
    "    data_info.loc[data_info.city_2 == k, 'lat'] = data_info.loc[data_info.city_2 == v, 'lat'].iloc[0]\n",
    "    data_info.loc[data_info.city_2 == k, 'lng'] = data_info.loc[data_info.city_2 == v, 'lng'].iloc[0]\n",
    "    data_info.loc[data_info.city_2 == k, 'population'] = data_info.loc[data_info.city_2 == v, 'population'].iloc[0]\n",
    "    data_info.loc[data_info.city_2 == k, 'density'] = data_info.loc[data_info.city_2 == v, 'density'].iloc[0]\n",
    "    data_info.loc[data_info.city_2 == k, 'city_2'] = v\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info.loc[data_info.city_2 == 'Huffman, TX', 'city_type'] = 'community'\n",
    "data_info.loc[data_info.city_2 == 'Summerfield, FL', 'city_type'] = 'community'\n",
    "\n",
    "data_info.loc[data_info.city_2 == 'Lakewood, NJ', 'lat'] = 40.0770\n",
    "data_info.loc[data_info.city_2 == 'Lakewood, NJ', 'lng'] = -74.1985\n",
    "data_info.loc[data_info.city_2 == 'Lakewood, NJ', 'population'] = 139506\n",
    "data_info.loc[data_info.city_2 == 'Lakewood, NJ', 'density'] = 5476.2\n",
    "\n",
    "data_info.loc[data_info.city_2 == 'Redford, MI', 'lat'] = 42.2341\n",
    "data_info.loc[data_info.city_2 == 'Redford, MI', 'lng'] = -83.1749\n",
    "data_info.loc[data_info.city_2 == 'Redford, MI', 'population'] = 49504\n",
    "data_info.loc[data_info.city_2 == 'Redford, MI', 'density'] = 4302.7\n",
    "\n",
    "data_info.loc[data_info.city_2 == 'Opa Locka, FL', 'lat'] = 25.5406\n",
    "data_info.loc[data_info.city_2 == 'Opa Locka, FL', 'lng'] = -80.1503\n",
    "data_info.loc[data_info.city_2 == 'Opa Locka, FL', 'population'] = 16463\n",
    "data_info.loc[data_info.city_2 == 'Opa Locka, FL', 'density'] = 3821.5\n",
    "\n",
    "data_info.loc[data_info.city_2 == 'Soddy Daisy, TN', 'lat'] = 35.1531\n",
    "data_info.loc[data_info.city_2 == 'Soddy Daisy, TN', 'lng'] = -85.1037\n",
    "data_info.loc[data_info.city_2 == 'Soddy Daisy, TN', 'population'] = 13070\n",
    "data_info.loc[data_info.city_2 == 'Soddy Daisy, TN', 'density'] = 564\n",
    "\n",
    "data_info.loc[data_info.city_2 == 'Saint Petersburg, FL', 'lat'] = 27.46\n",
    "data_info.loc[data_info.city_2 == 'Saint Petersburg, FL', 'lng'] = -82.38\n",
    "data_info.loc[data_info.city_2 == 'Saint Petersburg, FL', 'population'] = 261256\n",
    "data_info.loc[data_info.city_2 == 'Saint Petersburg, FL', 'density'] = 4175.0\n",
    "\n",
    "data_info.loc[data_info.city_2 == 'Mckinney, TX', 'lat'] = 33.1150\n",
    "data_info.loc[data_info.city_2 == 'Mckinney, TX', 'lng'] = -96.3823\n",
    "data_info.loc[data_info.city_2 == 'Mckinney, TX', 'population'] = 195308\n",
    "data_info.loc[data_info.city_2 == 'Mckinney, TX', 'density'] = 2974.0\n",
    "\n",
    "data_info.loc[data_info.city_2 == 'Port Saint Lucie, FL', 'lat'] = 27.1633\n",
    "data_info.loc[data_info.city_2 == 'Port Saint Lucie, FL', 'lng'] = -80.2118\n",
    "data_info.loc[data_info.city_2 == 'Port Saint Lucie, FL', 'population'] = 204851\n",
    "data_info.loc[data_info.city_2 == 'Port Saint Lucie, FL', 'density'] = 1718.3\n",
    "\n",
    "data_info.loc[data_info.city_2 == 'Saint Johns, FL', 'lat'] = 29.5775\n",
    "data_info.loc[data_info.city_2 == 'Saint Johns, FL', 'lng'] = -81.3146\n",
    "data_info.loc[data_info.city_2 == 'Saint Johns, FL', 'population'] = 86400\n",
    "data_info.loc[data_info.city_2 == 'Saint Johns, FL', 'density'] = -999\n",
    "data_info.loc[data_info.city_2 == 'Saint Johns, FL', 'city_type'] = 'community'\n",
    "\n",
    "data_info.loc[data_info.city_2 == 'Winston Salem, NC', 'lat'] = 36.0610\n",
    "data_info.loc[data_info.city_2 == 'Winston Salem, NC', 'lng'] = -80.1539\n",
    "data_info.loc[data_info.city_2 == 'Winston Salem, NC', 'population'] = 250001\n",
    "data_info.loc[data_info.city_2 == 'Winston Salem, NC', 'density'] = 1868.8\n",
    "\n",
    "data_info.loc[data_info.city_2 == 'Mcallen, TX', 'lat'] = 26.1259\n",
    "data_info.loc[data_info.city_2 == 'Mcallen, TX', 'lng'] = -98.1411\n",
    "data_info.loc[data_info.city_2 == 'Mcallen, TX', 'population'] = 142210\n",
    "data_info.loc[data_info.city_2 == 'Mcallen, TX', 'density'] = 2238\n",
    "\n",
    "data_info.loc[data_info.city_2 == 'Rotonda West, FL', 'lat'] = 26.5316\n",
    "data_info.loc[data_info.city_2 == 'Rotonda West, FL', 'lng'] = -82.1617\n",
    "data_info.loc[data_info.city_2 == 'Rotonda West, FL', 'population'] = 10114\n",
    "data_info.loc[data_info.city_2 == 'Rotonda West, FL', 'density'] = 949.05\n",
    "data_info.loc[data_info.city_2 == 'Rotonda West, FL', 'city_type'] = 'community'\n",
    "\n",
    "data_info.loc[data_info.city_2 == 'Placida, FL', 'lat'] = 26.4955\n",
    "data_info.loc[data_info.city_2 == 'Placida, FL', 'lng'] = -82.1554\n",
    "data_info.loc[data_info.city_2 == 'Placida, FL', 'population'] = -999\n",
    "data_info.loc[data_info.city_2 == 'Placida, FL', 'density'] = -999\n",
    "data_info.loc[data_info.city_2 == 'Placida, FL', 'city_type'] = 'community'\n",
    "\n",
    "data_info.loc[data_info.city_2 == 'Saint Louis, MO', 'lat'] = 38.3738\n",
    "data_info.loc[data_info.city_2 == 'Saint Louis, MO', 'lng'] = -90.1152\n",
    "data_info.loc[data_info.city_2 == 'Saint Louis, MO', 'population'] = 293310\n",
    "data_info.loc[data_info.city_2 == 'Saint Louis, MO', 'density'] = 4886.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "density       5483\n",
       "population    3864\n",
       "lat            369\n",
       "lng            369\n",
       "index            0\n",
       "zipcode          0\n",
       "state            0\n",
       "city             0\n",
       "city_state       0\n",
       "city_type        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_info.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info = data_info.drop(['state_full_name', 'city+state'], axis=1)\n",
    "data_info = data_info.rename(columns={'city_2': 'city_state'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = pd.read_csv('data/data_local_info.csv', dtype='object')\n",
    "\n",
    "# исправим ошибки индексов\n",
    "data_final = data_final[data_final.zipcode != '00000']\n",
    "data_final = data_final[data_final.zipcode != '05642']\n",
    "data_final = data_final[data_final.zipcode != '78697']\n",
    "\n",
    "data_final.loc[data_final.zipcode == '90109', 'zipcode'] = '98109'\n",
    "data_final.loc[data_final.zipcode == '98489', 'zipcode'] = '98498'\n",
    "data_final.loc[data_final.zipcode == '33249', 'zipcode'] = '33149'\n",
    "data_final.loc[data_final.zipcode == '22703', 'zipcode'] = '27703'\n",
    "data_final.loc[data_final.zipcode == '98798', 'zipcode'] = '98198'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработаем полученные координаты. Возьмём из них координаты с наибольшей значимостью:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coords_search(data):\n",
    "    if len(data) > 2:\n",
    "        number_of_coords = data.count('place_id')\n",
    "        coords_list = []\n",
    "        for c in range(number_of_coords):\n",
    "            idx_b = data.find('\\'lat\\'')\n",
    "            idx_e = data.find('display_name')\n",
    "            coords = data[idx_b-1:idx_e-3]\n",
    "            coords = re.sub(r'[^\\d.,-]', '', coords)\n",
    "            coords = coords.split(',')\n",
    "            coords = [float(el) for el in coords]\n",
    "            coords = tuple(coords)\n",
    "            data = data[(idx_e + 5):]\n",
    "            coords_list.append(coords)\n",
    "        if len(coords_list) > 0:\n",
    "            return coords_list[0]\n",
    "        else:\n",
    "            return np.NaN\n",
    "    else:\n",
    "        return np.NaN\n",
    "\n",
    "data_final['coords'] = data_final['location'].map(coords_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поправим адреса и попробуем ещё раз:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_parse_new = data_final[data_final.coords.isna()]\n",
    "\n",
    "# функция обрезает адрес до дом + улица\n",
    "def street_correction(data):\n",
    "    data = re.sub(r'St.+', 'St', data)\n",
    "    data = re.sub(r'Street.+', 'St', data)\n",
    "    data = re.sub(r'Rd.+', 'Rd', data)\n",
    "    data = re.sub(r'Road.+', 'Road', data)\n",
    "    data = re.sub(r'Ave.+', 'Ave', data)\n",
    "    data = re.sub(r'Avenue.+', 'Avenue', data)\n",
    "    data = re.sub(r'Lane.+', 'Lane', data)\n",
    "    data = re.sub(r'Cir.+', 'Cir', data)\n",
    "    data = re.sub(r'way.+', 'way', data)\n",
    "    data = re.sub(r'Way.+', 'way', data)\n",
    "    data = re.sub(r'Ter.+', 'Ter', data)\n",
    "    data = re.sub(r'Ct.+', 'Ct', data)\n",
    "    data = re.sub(r'Blvd.+', 'Blvd', data)\n",
    "    data = re.sub(r'Boulevard.+', 'Blvd', data)\n",
    "    data = re.sub(r'Sq.+', 'Sq', data)\n",
    "    data = re.sub(r'Dr.+', 'Dr', data)\n",
    "    data = re.sub(r'Drive.+', 'Drive', data)\n",
    "    return data  \n",
    "\n",
    "data_to_parse_new = data_to_parse_new.reset_index()\n",
    "data_to_parse_new['street_new'] = data_to_parse_new['street'].map(street_correction)\n",
    "data_to_parse_new.loc[keyword_mask(data_to_parse_new, 'street_new', ['isclosed']), 'street_new'] = 'no_address'\n",
    "# data_to_parse_new.to_csv('data/data_to_parse_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_info.to_csv('data/data_city_info.csv')\n",
    "# data_final.to_csv('data/data_local_info.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
